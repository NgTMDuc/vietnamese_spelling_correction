{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqeXtfqyIUkU",
        "outputId": "09d59cbc-b32d-4cd3-fec6-889cb07668e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Vietnamese-Corrector'...\n",
            "remote: Enumerating objects: 42, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 42 (delta 7), reused 10 (delta 4), pack-reused 26\u001b[K\n",
            "Unpacking objects: 100% (42/42), 54.19 MiB | 5.31 MiB/s, done.\n",
            "Updating files: 100% (21/21), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/bmd1905/Vietnamese-Corrector.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Vietnamese-Corrector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se-wX7sFIZFh",
        "outputId": "f8314d34-d5e7-471a-97cd-071028f30f39"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Vietnamese-Corrector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt unidecode"
      ],
      "metadata": {
        "id": "lx8XQhZhIm_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!sh convert_leipzig_data.sh"
      ],
      "metadata": {
        "id": "RQNOMthMIest"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate_dataset.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_5pUcnZI0bk",
        "outputId": "ddcfcb2c-d032-4bca-a3cb-faa4cc5ad5bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-15 12:16:03.926590: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-15 12:16:04.924393: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 12:16:04.924492: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 12:16:04.924511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Downloading (…)okenizer_config.json: 100% 408/408 [00:00<00:00, 61.7kB/s]\n",
            "Downloading (…)\"spiece.model\";: 100% 4.31M/4.31M [00:00<00:00, 10.2MB/s]\n",
            "Downloading (…)\"tokenizer.json\";: 100% 16.3M/16.3M [00:00<00:00, 74.9MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 65.0/65.0 [00:00<00:00, 6.18kB/s]\n",
            "  1% 4393/479568 [00:04<10:40, 742.06it/s]skipping line as its too long (2236):\n",
            "Ti cng c hiu bit thm v phong tc cc nc cng nh nt c trng ca cc trang phc ci chu , b ng Minh H, i din ca Qu gallery, tm s.. Gim c chng trnh l hi Susan Lin cho bit Ti  n Vit Nam cch y hai thng, vn ho v truyn thng ca ngi Vit thc s c nhng nt rt c trng khng nh chng ta vn ngh l nh hng nhiu ca vn ho Trung Hoa. Nhng va chuyn sang lnh vc thit k thi trang c mt thi gian, b bu Victoria  gi tn hiu gii ho n Geri v mong mun h li c tnh bn thn thit nh xa. Nhng thit k c o v cc k l mt c trnh din trong tun l thi trang ny  khin gii thi trang kinh ngc v thch th. Trong bi cnh Nam B t khi thc dn Php bt u xm lc n cuc Cch mng Thng Tm 1945, Di c i ngha phc tho gc nhn kh bao qut v cc lc lng t pht t khi cn hot ng v quyn li cc b n lc tt c cng tp hp di ngn c i ngha do ng Cng sn Vit Nam lnh o.. Hai din vin Trung Dng (gia) v Trng Minh Quc Thi (phi)  gp phn lm cho b phim thnh cng - nh T.F.SKhng ch to nhng im nhn qua cch nhn su sc v cc nhn vt lch s c tht nh Mi Tr, By Vin, Tm Mnh, Di c i ngha t n mt chiu su nhn bn khi khc ha nhn vt m ng. Ngay lp tc ngi ph n ku ln Triu Vi nh ngi v ln xe vo bnh vin 306. Marlon Brando - din vin ng gangster t nht. nh rng y  trong phong tro ca thanh nin, sinh vin cc thnh ph ln cng xut hin nhng c nhn, ti nng nhng qu thc, mt bi cnh cn nhiu kh khn lc by gi cha c ai m mng xa xi rng 10 nm sau s c phong tro mnh m vi khi nim rock Vit.. Vy , c cho rng qua bi cnh b bn ca nhc tr lc by gi th cu ni gn 15 nm trc nh ca mt k ngoi o  v tnh gn vi ti nh mt nh mnh. iu mi nht v cng nm trong d on ca nhiu ngi,  l Cnh diu vng nm nay l s tn vinh nhng thnh cng ca cc ngh s tr Ng Quang Hi, Bi Thc Chuyn, Hi Yn, Trn Hng, Trn Hu Phc.... Nhng ngh s n anh, nhng ngi  ng gp khng nh cho s pht trin ca nn in nh cch mng Vit Nam cng c ng o cc ngh s tng nh, tn vinh, mt truyn thng p ca cc l trao gii thng Cnh diu vng ngh s c Hon, ngh s Phm K Nam, ngh s Khng M.... in nh Vit Nam hm nay vn cn phi i mt vi rt nhiu kh khn, th thch nhng mt m tn vinh cc ngh s hng nm s l ngun ng vin khng th thiu cho nhng cm hng sng to, lao ng ngh thut, cng s l mt hot ng vn ha c ng o cng chng n i.. 22 gii thng gm. Con gi ln Ga Young ca b - mt phng vin, 28 tui - rt chn chn trong suy ngh. Vi chiu cao ny, Kim T Thp Cheops l cng trnh cao nht th gii trong khong thi gian 43 th k.\n",
            " 21% 102038/479568 [01:53<05:44, 1097.24it/s]skipping line as its too long (2201):\n",
            "iu ny cho thy kh nng hnh hc nm sn  trong no.. Ngi ln v tr em u c nhng khi nim r rng nh u l tm im ca hnh trn v s ko di logic ca mt ng thng, bt chp vic  c nhng khi nim ny hay cha.. Gio s Stanislas Dehaene ti i hc Php  Paris v cng s  kim tra 14 tr em v 30 ngi ln thuc nhm ngi Amazon, gi l Munduruku, v so snh vi kt qu tm c trn tr em v ngi ln M.. Nhm ngi Munduruku vn dng mt cch t nhin nhng khi nim hnh hc c bn nh im, ng thng, s song song, gc  pht hin s khc l trong nhng bc hnh n gin, v h s dng c khong cch, gc hnh v mi tng quan ca chng trong cc bn  hnh hc,  nh v nhng vt th b giu kn, nhm cho bit.. Kt qu ca chng ti  cung cp bng chng v s tn ti ca trc gic hnh hc, khi cha h tri qua trng lp, hay tip xc vi nhng biu tng, bn  v c kho thut ng hnh hc phong ph.. Hnh hc cng l mt lnh vc khoa hc c i v nhm ca Dehaene cho rng n bt ngun t kh nng bm sinh.. Rt nhiu nh l trong hnh hc - nh 2 im to nn mt ng thng, 3 trc giao nhau to nn mt im u c coi l iu hin nhin v cha h c cht vn v s logic, nguyn l v c th nghim, cc nh nghin cu ni.. Khng c chuyn nhng ngi Munduruku  hc v nhng khi nim ny. Anh  tn dng mn qu tnh c tri ph   kh luyn v mong mt ngy, cng chng v gii yu ngh thut nh n ci tn inh Gia L.. . Nhc s Quc Bo hy xem li nhn cch ca mnh, cn H Kiu Anh, ch lun mi l nhn vt v ti trong v n ng Nam theo cch nh gi ca ch, nhng bn trong s vic, ch t hn bit r hn ai ht. Ngi sao 23 tui kin quyt gi v oan trang bng cch yu cu o din Mike Nichols ct nhng cnh nude  quay trong b phim sp ra mt Closer.. Din vin Natalie Portman.. Nhng cnh gi cm  c hon thnh ti mt cu lc b gn phim trng v Nichols t ra rt ng . Nhng sau khi xem li phim, Natalie  nn n o din ct b cnh h hang ca c v cui cng,  ngh ny  c ng chp nhn.. Natalie ni ting d ng vi nhng cnh khoe hnh th trn mn bc. Nguyn Khc Trng 'Vn hc Vit Nam thiu bn nng'. Tuy nhin,  i ch, do mun phc tho bi cnh thi i qu rng nn phim khng trnh khi dn tri. Tng cng vic lm n n nh, d u nm 1917 c mt ngi Hoa tn bang Tc (tn tht l M Ngn)  dng mu chim ot t ca gia nh b. inh Gia L v nim hoi nh v th gii t nhin. Nhng bn cnh nhng nhc s nh th cn nhiu tc gi tr t cho php mnh d di, ngi ht cng nh con vt, khng  tm n  ngha cu c, li ht ni v ci g nn cui cng mt sn phm vn ha tinh thn m chng em li gi tr no cho ngi thng thc.\n",
            " 35% 169938/479568 [03:09<04:56, 1043.59it/s]skipping line as its too long (6085):\n",
            "H Hu- o to H h chnh quy 5.900 h va hc va lm 5.200 h c tuyn 200- o to khc bng hai 780 hon chnh kin thc 765 i hc  Nng- o to H h chnh quy 6.000 h va hc va lm 4.300 h c tuyn 90.- o to C h chnh quy 1.200 h va hc va lm 300- o ti khc bng hai 1.250 hon chnh kin thc 3.000 Trng H Ngoi ng- o to H h chnh quy 1.400 h va hc va lm 2.800- o to khc bng hai 250 Trng H Ty Bc- o to H h chnh quy 1.200 h va hc va lm 700 h c tuyn 100- o to C h chnh quy 300 h va hc va lm 300- o to d b 100 Trng H Vinh- o to H h chnh quy 2.780 h va hc va lm 2.500 h c tuyn 60 (trong  phn hiu H Tnh h chnh quy 250 h va hc va lm 250) Trng H Ty Nguyn- o to H h chnh quy 1.830 h va hc va lm 1.000 h c tuyn 100- o to d b 100 Trng H Quy Nhn- o to H h chnh quy 2.800 h va hc va lm 2.200 h c tuyn 50.- o to khc bng hai 500 hon chnh kin thc 1.200 Trng H  Lt- o to H h chnh quy 2.800 h va hc va lm 1.800 h c tuyn 60.- o to C h chnh quy 400- o to khc bng hai 500 hon chnh kin thc 800 Trng H Cn Th - o to H h chnh quy 5.200 h va hc va lm 1.700 h c tuyn 100.- o to C h chnh quy 100- o to d b 80- o to khc bng hai 200 hon chnh kin thc 400 Trng H Bch khoa H Ni- o to H h chnh quy 3.870 h va hc va lm 2.300- o to C h chnh quy 2.500- o to khc bng hai 1.200 Trng H Xy dng H Ni- o to H h chnh quy 2.800 h va hc va lm 1.800 h c tuyn 50- o to khc bng hai 400 Trng H Giao thng vn ti HN- o to H h chnh quy 3.150 h va hc va lm 2.300 h c tuyn 50- o to khc bng hai 300 hon chnh kin thc 400 Trng H M - a cht- o to H h chnh quy 200 h va hc va lm 1.800- o to C h chnh quy 300- o to khc bng hai 250 hon chnh kin thc 150 Trng H M thut cng nghip- o to H h chnh quy 120 h va hc va lm 200- o to C h chnh quy 150- o to khc hon chnh kin thc 50 Trng H Lut TP H Ch Minh- o to H h chnh quy 990 h va hc va lm 1.100 h c tuyn 30- o to khc bng hai 500 Trng H S phm H Ni- o to H h chnh quy 2.450 h va hc va lm 3.700- o to khc bng hai 1.000 hon chnh kin thc 2.000 Trng H S phm H Ni 2- o to H h chnh quy 1.500 h va hc va lm 1.000 h c tuyn 50- o to khc bng hai 200 hon chnh kin thc 700 Trng HSP TDTT H Ty- o to H h chnh quy 400 h va hc va lm 400 h c tuyn 30- o to C h chnh quy 250 Trng H SP k thut Hng Yn- o to H h chnh quy 700 h va hc va lm 600 h c tuyn 30- o to C h chnh quy 650 h va hc va lm 400- o to khc hon chnh kin thc 250 Trng HSP k thut TP.HCM- o to H h chnh quy 2.450 h va hc va lm 1.500 h c tuyn 30- o to C h chnh quy 300- o to d b 25- o to khc hon chnh kin thc 500 Trng HSP TP.HCM- o to H h chnh quy 2.500 h va hc va lm 2.200- o to khc bng hai 200 hon chnh kin thc 1.500 Trng HSP TDTT TP.HCM- o to H h chnh quy 200 h va hc va lm 200 h c tuyn 30- o to C h chnh quy 300 h va hc va lm 200 Trng HSP ng Thp- o to H h chnh quy 1.650 h va hc va lm 800- o to C h chnh quy 800- o to khc bng hai 300 hon chnh kin thc 300 Trng H Kinh t quc dn HN- o to H h chnh quy 3.770 h va hc va lm 4.200 h c tuyn 40- o to khc bng hai 2.200 hon chnh kin thc 2.500 Trng H Kinh t TP.HCM- o to H h chnh quy 5.000 h va hc va lm 3.500- o to d b 25- o to khc bng hai 3.200 hon chnh kin thc 3.500 Trng H Thng mi- o to H h chnh quy 2.400 h va hc va lm 2.300- o to C h chnh quy 300- o to khc bng hai 600 hon chnh kin thc 1.000 Trng H Ngoi thng- o to H h chnh quy 2.300 h va hc va lm 2.000- o to C h chnh quy 300- o to khc bng hai 1.400 hon chnh kin thc 700 Trng H Nng nghip 1- o to H h chnh quy 2.770 h va hc va lm 2.000- o to khc bng hai 150 hon chnh kin thc 200 Trng H Nng- Lm TP.HCM- o to H h chnh quy 3.300 h va hc va lm 2.800 h c tuyn 40- o to C h chnh quy 360- o to d b 25- o to khc bng hai 360 Trng H Thy sn Nha Trang- o to H h chnh quy 2.000 h va hc va lm 1.800- o to C h chnh quy 450- o to khc bng hai 200 Vin H M H Ni- o to H h chnh quy 2.500 h va hc va lm 1.600- o to C h chnh quy 300- o to khc bng hai 800 hon chnh kin thc 800 H M bn cng TP.HCM- o to H h chnh quy 2.820 h va hc va lm 850- o to C h chnh quy 300- o to khc bng hai 2.000 Trng CSP Nhc ha T. - o to H h va hc va lm 50- o to C h chnh quy 800 h va hc va lm 400 Trng CSP NT- MG T.1- o to C h chnh quy 1.000 h va hc va lm 1.000 Trng CSP NT- MG T.2- o to C h chnh quy 500 h va hc va lm 500 Trng CSP Mu gio T.3- o to C h chnh quy 600 h va hc va lm 750 Trng d b H Dn tc T.- o to d b 550 Trng d b H Dn tc Sm Sn- o to d b 430 Trng d b H T. Nha Trang- o to d b 500 Trng d b H TP H Ch Minh- o to d b 540 Trng ph thng vng cao Vit Bc- o to d b 300Cc trng dn lp, t thc Trng H dn lp Thng Long- H chnh quy 1.000 Trng H dn lp Phng ng- H chnh quy 1.700 Trng H dn lp ng - H chnh quy 1.100 Trng H dn lp QL- KD H Ni- H chnh quy 1.600 Trng H dn lp Duy Tn - H chnh quy 1.500 Trng H dn lp Hng Vng - H chnh quy 1.000 Trng H dn lp Ngoi ng - Tin hc- H chnh quy 1.500 Trng H dn lp Vn Lang - H chnh quy 2.000 Trng H dn lp K thut cng nghip TP.HCM- H chnh quy 1.700 h khng chnh quy 300 Trng H dn lp Hi Phng- H chnh quy 1.500 Trng H dn lp Vn Hin- H chnh quy 800 Trng H dn lp Bnh Dng- H chnh quy 1.400 h khng chnh quy 300 Trng H dn lp Hng Bng- H chnh quy 1.700 h khng chnh quy 300 Trng H dn lp Lc Hng - H chnh quy 1.550 h khng chnh quy 300 Trng H dn lp Cu Long - H chnh quy 1.500 Trng H dn lp Ph Xun- H chnh quy 1.300 Trng H dn lp Lng Th Vinh- H chnh quy 1.500 h khng chnh quy 400 Trng HDL Cng ngh Si Gn- H chnh quy 1.400 h khng chnh quy 300 Trng H dn lp Yersin- H chnh quy 1.100 Trng C dn lp ng Du- H chnh quy 1.000 Trng CDL KT- KT Bnh Dng- H chnh quy 1.000 Trng CDL Cng ngh thng tin- H chnh quy 1.500 Trng C t thc Thnh - H chnh quy 1.000 Trng C t thc c Tr - H chnh quy 900 Trng C t thc Nguyn Tt Thnh - H chnh quy 900 Trng C t thc KT- CN TP H Ch Minh- H chnh quy 900 Trng C t thc KT- CN ng Nai- H chnh quy 800. . Nhng gi th ti  t tin hn nhiu vo kh nng m nhc ca mnh. Anh tip tc thc hin mt b phim na v tui hc tr? c bit lc ti ri vo vng th phi, h rt tin tng ti, khch l ti. Cc gii thng c nhn . Theo ti bit, trong chng trnh  cng c mt s ngh s khng d duyt nhng vn c din.. - Ch s chp nhn hnh thc k lut nh th no?. Tc gi ngi o ot gii Nobel vn hc 2004. Nhng  trt ngi trn lng cp ri th phi ci thi. Cuc chin u mt mt mt cn . Nhng cng v sau ti cng thy anh chng ny thch ht nhp hn l ht tht.. Ngi gi Trng Quc Thanh, Gi ti Ban Vn hoTiu  Vit Quang c ht lip sync hay khng?\n",
            " 41% 196050/479568 [03:38<04:29, 1052.46it/s]skipping line as its too long (3049):\n",
            "y l mt trong nhng bin php thit thc ngn chn tham nhng pht sinh ngay t cp c s.. Theo , cn b thuc S khng c pht ngn v thc hin tri vi quy nh ca Nh nc v cc vn bn ch o ca S chy li, trn trnh trch nhim hoc thoi thc, n y cng vic cho nhau tit l thng tin, ti liu mt, cc loi vn bn cha c php cng khai x l mt vn  nghip v - k thut php l m bit rng mnh khng  kh nng hoc thiu chun b trong iu kin c th x l vn bn h s chm so vi thi gian cho php nh thm nh, cp php, vn bn ch o, tr li cng dn  xut trin khai khng kp thi cc ch o ca cp trn c tnh trnh cc vn bn vt thm quyn hoc c ni dung tri quy nh php lut c  gii quyt cng vic ko di qu thi hn quy nh hoc gii quyt cng vic qu nhanh vt mc bnh thng m b qua cc th tc quy nh quan liu thit trch nhim kim tra, theo di cng vic thuc trch nhim ca mnh i cng tc c s gp  ty tin ngoi phm vi trng phng giao.... i vi doanh nghip qun l xy dng, ng Dng cho bit, nhng cn b nhn vin ny khng c  h s ti liu tht lc, b tr cn b xy dng c bn thiu trung thc, km o c, khng  nng lc chuyn mn mc ngoc vi cc n v t vn v nh thu nhm mc ch t li c nhn x nh gi thu hoc khi lng  ch nh thu t vn hoc thu xy lp m khng u thu theo quy nh lp bin bn thi cng, quy m cng trnh m ch nhm vo nh thu quen bit trc m loi b nhng nh thu khc c my mc thit b v nng lc thi cng ph hp hn.. Ngoi ra, khi xem xt cc thit b thi cng c chng vo h s d thu  t im k thut, ch u t phi kim tra, phn tch kh nng v hiu qu my mc thit b s dng trong thc t i chiu trong h s, tuyt i khng c la chn thiu c s khng c lp d ton cao hn quy nh, k khng, bt xn khi lng vt t t  lp h s pht sinh tng khi lng so vi thit k c duyt thanh ton khi lng sai thc t thi cng hoch ton sai ch  qun l ti chnh theo quy nh thc hin n gi nh mc thi cng loi vt t khng ng chng loi v tnh sai nh mc  c quy nh khon trng cho t vn gim st, kho st thit k, theo di thi cng, khng lp k hoch b tr cn b bm st cng trng chm im u thu thin v v thay i tiu chun thu c li cho bn d thu.. Ngc li, n v t vn thit k khng c lp bin php thi cng ln hn so vi nhu cu thc t nhm tng kinh ph cng trnh v lp d n thit k k thut thi cng- d ton khng khi lng so vi thc t hoc khng c t  lp h s pht sinh tng khi lng- ng Dng ni.. Ring i vi nhng doanh nghip xy lp cng khng c chy cht, lun lch  c giao thu khng ng quy nh t  thay i vt t thi cng khng m bo cht lng hoc thay i thit k k thut nhm tng khi lng thi cng nhn qu nhiu cng trnh vt qu kh nng ca n v hoc bn, nhng nhng thu cho cc n v khc khon trng cho cc n v thi cng hoc cho c nhn, t chc c quan bn ngoi n v mn php nhn  thc hin cc cng trnh.. Mc d c s ng tnh ng h ca a s nhn vin trong S Giao thng cng chnh, nhng quy nh ny vn phi sa i mt vi chi tit nh v t ng, cng nh thm quy nh  ph hp vi c th ca cc doanh nghip cng ch v ch ti x l cc vi phm.. . V thc s, ti  bc l c tnh ca mnh, kin quyt vt qua mi lc cn  ln hc trng Ngh thut Vit Bc. Ba vng ca h u bng nhau, nn khi mc qun o nhn nh cy ch di ng. Dn lng Nc Nn k, m m b thng ra ngha trang tr chuyn vi anh em, con chu. Khi vo vai ng ni thng Bm trong Thng Bm, ti phi hc cch sng ca ng gi xa kh tnh ra sao, v u m ng ta phi sng nh vy? Huyn thoi m nhc tit l, anh  dnh vi nng tin nu t thi cn l mt thnh vin ca T qui.\n",
            " 43% 207179/479568 [03:51<04:27, 1018.39it/s]skipping line as its too long (2886):\n",
            "n hm qua 23.3, B GD-T mi b sung 2 trng khng t chc thi l C S phm H Ni v C S phm y t Hu.. Phn c iu chnh nhiu nht l phn 2, c th k ra nh sau trang 31 b tt c cc dng 17, 18, 19 (t trn xung)  in thuc cc ct 2, 4 v 5 ngnh S phm sinh - k thut nng nghip Trng C S phm ng Nai thi khi B ch khng phi thi khi A (trang 184) ngnh Qun l gio dc ca Trng H S phm TP.HCM (trang 140) ch tuyn th sinh l gio vin hoc cn b qun l (thi khi A, C, D1), ngnh Vit Nam hc thi khi C, D1 Trng H M-a cht t chc o to ngnh M ti Qung Ninh vi 200 ch tiu (thay v 150 ch tiu - trang 45) tng ch tiu Trng H S phm k thut Nam nh l 1.300 (thay v 1.100 - trang 53) ngy thi ca Trng C S phm Hi Dng v Trng C S phm Sn La t 16 - 21.7 (thay v 16 - 22.7, trang 93 v 98) ngnh M thut to hnh (hnh ha, iu khc) Trng H dn lp Hng Bng thi khi H v V (trang 152) ngnh Ting Trung Trng H dn lp Ph Xun thi khi D1 v D4 (thay v D1 v D3), ngnh Ting Php thi khi D1 v D3 (thay v khi D1 v D4 - trang 156) ngnh Vit Nam hc Trng C t thc k thut - cng ngh ng Nai thi khi C - D1, 2, 3, 4 (thay v khi A, D 1, 2, 3, 4 - trang 156) ngnh 01 S phm m nhc Trng C S phm C Mau thi khi N (thay v khi A), ngnh S phm m thut thi khi H (thay v khi B - trang 178) m ngnh ca ngnh Th vin - Thng tin (ngoi s phm) l 10 (thay v 09), K ton (ngoi s phm) l 12 (thay v 10), Ting Anh (chuyn ngnh Thng mi - Du lch, ngoi s phm) l 13 (thay v 12), Tin hc (chuyn ngnh Qun l mng, ngoi s phm) l 14 (thay v 13 - trang 193) ngnh Ch huy tham mu phng khng Hc vin phng khng - khng qun thi khi A (thay v khng c khi thi - trang 198) ngnh Cng ngh phn mm v ngnh Mng my tnh thi khi A v D1 (thay v C v D1 - trang 168) Trng C in lc TP.HCM ch tuyn sinh ngnh H thng in (thay v  7 ngnh, trang 164) Trng C cng ng Tr Vinh c vng tuyn gm c th sinh c h khu thng tr ti cc tnh ng bng sng Cu Long (thay v ch tuyn th sinh c h khu thng tr  tnh Tr Vinh - trang 163) ngnh Qun tr vn phng Trng C bn cng Hoa Sen ch t chc thi khi A v D1 (khng thi khi C nh trong sch  ghi), b sung tuyn sinh ngnh mi l Qun tr du lch - khch sn nh hng (khi thi A v D1, m ngnh 08 - trang 160).. B sung bng Cng thng tin thi v tuyn sinh. Gn y nht, gim c Trng Thc i - vai din dnh huy chng vng ti Hi din Sn khu kch ni ton quc va din ra -  em li cho anh nhng pht giy vinh quang thc s. Ngi n ng duy nht cn li khin cho c tin li khng thuc v th gii ca hc thc, khng phi nhng k vy quanh c m chnh l Ho, ngi n ng tng ra t vo khm. B su tp thi trang 'Rong chi ma h'. Khng ng sau khi ng th mt vi tiu phm, o din  cho ti c hi qu bu ny.. - Khi vo vai Si, anh b p lc ra sao?. Mt bn sc vn ha ngn i ca cng ng cc dn tc Ty Nguyn ang c c th gii tn vinh.... Nhng ngy ny, c Ty Nguyn ang bng ln trong khng gian l hi. Cng l din vin cng hay ng chm. V Bng sen vng nm nay, nh ti  ni vi Thanh Vn sau khi Vn mi ti xem duyt hnh Ngi n b mng du Phim ny cn thiu khong 3 cun na  cho cc nhn vt tr thnh nhn vt, nh vai ca V Hoi Nam ng ch l qun chng c din xut. Phim hot hnh Khng c gii Cnh diu vng!\n",
            "100% 479568/479568 [08:54<00:00, 897.56it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/Vietnamese-Corrector/vi.csv /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "O57MGm_R7x_L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/Vietnamese-Corrector/vi.test.csv /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "6LCHLhhW76Gs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/Vietnamese-Corrector/vi.train.csv /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "nTOIBjy976KC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/vi.test.csv /content/Vietnamese-Corrector"
      ],
      "metadata": {
        "id": "rOvhUrDe8krF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/vi.train.csv /content/Vietnamese-Corrector"
      ],
      "metadata": {
        "id": "AAvhLhe98qf2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/vi.csv /content/Vietnamese-Corrector"
      ],
      "metadata": {
        "id": "4dPu75aX8rnY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "kVp4GYazJIUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sh train_bart_model.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI7z72GBI3QX",
        "outputId": "94ff6ff5-a7b8-4494-9d51-c4f38c99b39d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-15 12:26:26.932052: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-15 12:26:27.878506: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 12:26:27.878625: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 12:26:27.878643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=steps,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=32,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0004,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./models/my-bart-base-en-mix/runs/Feb15_12-26-33_652da2e391e6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=./models/my-bart-base-en-mix/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=32,\n",
            "per_device_train_batch_size=8,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./models/my-bart-base-en-mix/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-9817efb164e4de26\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/csv\n",
            "INFO:datasets.builder:Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-9817efb164e4de26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-9817efb164e4de26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 9279.43it/s]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1548.86it/s]\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-9817efb164e4de26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 198.21it/s]\n",
            "Downloading (…)lve/main/config.json: 100% 744/744 [00:00<00:00, 117kB/s]\n",
            "[INFO|configuration_utils.py:660] 2023-02-15 12:26:42,596 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--doc2query--msmarco-vietnamese-mt5-base-v1/snapshots/1ac7b8c530c4dcbce052a0f1b7c2beca48ad21f5/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-15 12:26:42,599 >> Model config MT5Config {\n",
            "  \"_name_or_path\": \"doc2query/msmarco-vietnamese-mt5-base-v1\",\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-15 12:26:42,734 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--doc2query--msmarco-vietnamese-mt5-base-v1/snapshots/1ac7b8c530c4dcbce052a0f1b7c2beca48ad21f5/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-15 12:26:42,734 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--doc2query--msmarco-vietnamese-mt5-base-v1/snapshots/1ac7b8c530c4dcbce052a0f1b7c2beca48ad21f5/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-15 12:26:42,734 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-15 12:26:42,734 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--doc2query--msmarco-vietnamese-mt5-base-v1/snapshots/1ac7b8c530c4dcbce052a0f1b7c2beca48ad21f5/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-15 12:26:42,734 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--doc2query--msmarco-vietnamese-mt5-base-v1/snapshots/1ac7b8c530c4dcbce052a0f1b7c2beca48ad21f5/tokenizer_config.json\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 2.33G/2.33G [00:42<00:00, 54.2MB/s]\n",
            "[INFO|modeling_utils.py:2275] 2023-02-15 12:27:26,751 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--doc2query--msmarco-vietnamese-mt5-base-v1/snapshots/1ac7b8c530c4dcbce052a0f1b7c2beca48ad21f5/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:543] 2023-02-15 12:27:28,287 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2857] 2023-02-15 12:27:35,715 >> All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:2865] 2023-02-15 12:27:35,715 >> All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at doc2query/msmarco-vietnamese-mt5-base-v1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n",
            "[INFO|modeling_utils.py:2522] 2023-02-15 12:27:35,857 >> Generation config file not found, using a generation config created from the model config.\n",
            "Running tokenizer on train dataset:   0% 0/478 [00:00<?, ?ba/s]/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-9817efb164e4de26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ecd75429b454edd7.arrow\n",
            "Running tokenizer on train dataset: 100% 478/478 [07:13<00:00,  1.10ba/s]\n",
            "Running tokenizer on validation dataset:   0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-9817efb164e4de26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fb7fe9bc9549976b.arrow\n",
            "Running tokenizer on validation dataset: 100% 2/2 [00:01<00:00,  1.45ba/s]\n",
            "Downloading builder script: 100% 5.60k/5.60k [00:00<00:00, 4.99MB/s]\n",
            "[INFO|trainer.py:565] 2023-02-15 12:34:59,479 >> Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1650] 2023-02-15 12:34:59,503 >> ***** Running training *****\n",
            "[INFO|trainer.py:1651] 2023-02-15 12:34:59,504 >>   Num examples = 477563\n",
            "[INFO|trainer.py:1652] 2023-02-15 12:34:59,504 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1653] 2023-02-15 12:34:59,504 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1654] 2023-02-15 12:34:59,504 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "[INFO|trainer.py:1655] 2023-02-15 12:34:59,504 >>   Gradient Accumulation steps = 32\n",
            "[INFO|trainer.py:1656] 2023-02-15 12:34:59,504 >>   Total optimization steps = 5595\n",
            "[INFO|trainer.py:1657] 2023-02-15 12:34:59,506 >>   Number of trainable parameters = 582382848\n",
            "  0% 0/5595 [00:00<?, ?it/s][WARNING|logging.py:281] 2023-02-15 12:34:59,590 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Traceback (most recent call last):\n",
            "  File \"run_summarization.py\", line 708, in <module>\n",
            "    main()\n",
            "  File \"run_summarization.py\", line 627, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1543, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1791, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2539, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2571, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/mt5/modeling_mt5.py\", line 1695, in forward\n",
            "    decoder_outputs = self.decoder(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/mt5/modeling_mt5.py\", line 1026, in forward\n",
            "    layer_outputs = layer_module(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/mt5/modeling_mt5.py\", line 602, in forward\n",
            "    hidden_states = self.layer[-1](hidden_states)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/mt5/modeling_mt5.py\", line 195, in forward\n",
            "    forwarded_states = self.DenseReluDense(forwarded_states)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/mt5/modeling_mt5.py\", line 166, in forward\n",
            "    hidden_gelu = self.act(self.wi_0(hidden_states))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/activations.py\", line 35, in forward\n",
            "    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 14.76 GiB total capacity; 13.84 GiB already allocated; 3.88 MiB free; 13.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "  0% 0/5595 [00:03<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iCBC8q6pGpvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#facebook/bart-base\n",
        "#doc2query/msmarco-vietnamese-mt5-base-v1\n",
        "!python run_summarization.py \\\n",
        "    --model_name_or_path facebook/bart-base \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --evaluation_strategy=\"steps\" \\\n",
        "    --eval_steps=500 \\\n",
        "    --train_file vi.train.csv \\\n",
        "    --validation_file vi.test.csv \\\n",
        "    --output_dir ./models/my-bart-base-en-mix/ \\\n",
        "    --overwrite_output_dir \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=16 \\\n",
        "    --gradient_accumulation_steps=16 \\\n",
        "    --learning_rate=\"4e-4\" \\\n",
        "    --num_train_epochs=1 \\\n",
        "    --predict_with_generate \\\n",
        "\t--logging_steps=\"10\" \\\n",
        "    --save_total_limit=\"2\" \\\n",
        "    --max_target_length=1024 \\\n",
        "    --max_source_length=1024 \\\n",
        "    --fp16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trXPWAuZFmrB",
        "outputId": "c82a3850-583e-4ce8-ca36-32bbb641e334"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-15 12:44:50.777640: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-15 12:44:52.059359: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 12:44:52.059510: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-15 12:44:52.059533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=steps,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=16,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0004,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./models/my-bart-base-en-mix/runs/Feb15_12-44-55_652da2e391e6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=./models/my-bart-base-en-mix/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./models/my-bart-base-en-mix/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-9817efb164e4de26\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/csv\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-9817efb164e4de26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-9817efb164e4de26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-9817efb164e4de26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
            "100% 2/2 [00:00<00:00, 164.97it/s]\n",
            "Downloading (…)lve/main/config.json: 100% 1.72k/1.72k [00:00<00:00, 262kB/s]\n",
            "[INFO|configuration_utils.py:660] 2023-02-15 12:44:56,432 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-15 12:44:56,436 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:458] 2023-02-15 12:44:56,563 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:660] 2023-02-15 12:44:56,697 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-15 12:44:56,698 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 4.95MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 3.49MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 7.68MB/s]\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-15 12:44:58,438 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-15 12:44:58,438 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-15 12:44:58,438 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-15 12:44:58,438 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-15 12:44:58,438 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-02-15 12:44:58,438 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:660] 2023-02-15 12:44:58,439 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-15 12:44:58,439 >> Model config BartConfig {\n",
            "  \"_name_or_path\": \"facebook/bart-base\",\n",
            "  \"activation_dropout\": 0.1,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_bias_logits\": false,\n",
            "  \"add_final_layer_norm\": false,\n",
            "  \"architectures\": [\n",
            "    \"BartModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classif_dropout\": 0.1,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"bart\",\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"normalize_before\": false,\n",
            "  \"normalize_embedding\": true,\n",
            "  \"num_beams\": 4,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 128,\n",
            "      \"min_length\": 12,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_cnn\": {\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 142,\n",
            "      \"min_length\": 56,\n",
            "      \"num_beams\": 4\n",
            "    },\n",
            "    \"summarization_xsum\": {\n",
            "      \"length_penalty\": 1.0,\n",
            "      \"max_length\": 62,\n",
            "      \"min_length\": 11,\n",
            "      \"num_beams\": 6\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 558M/558M [00:03<00:00, 150MB/s]\n",
            "[INFO|modeling_utils.py:2275] 2023-02-15 12:45:02,425 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--bart-base/snapshots/aadd2ab0ae0c8268c7c9693540e9904811f36177/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:543] 2023-02-15 12:45:02,743 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2857] 2023-02-15 12:45:05,313 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:2865] 2023-02-15 12:45:05,313 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
            "[INFO|modeling_utils.py:2522] 2023-02-15 12:45:05,472 >> Generation config file not found, using a generation config created from the model config.\n",
            "Running tokenizer on train dataset:   0% 0/478 [00:00<?, ?ba/s]/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-9817efb164e4de26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-105216c0c300e879.arrow\n",
            "Running tokenizer on train dataset: 100% 478/478 [07:22<00:00,  1.08ba/s]\n",
            "Running tokenizer on validation dataset:   0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-9817efb164e4de26/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-65fd220cd7ba5528.arrow\n",
            "Running tokenizer on validation dataset: 100% 2/2 [00:01<00:00,  1.24ba/s]\n",
            "[INFO|trainer.py:565] 2023-02-15 12:52:31,491 >> Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1650] 2023-02-15 12:52:31,499 >> ***** Running training *****\n",
            "[INFO|trainer.py:1651] 2023-02-15 12:52:31,499 >>   Num examples = 477563\n",
            "[INFO|trainer.py:1652] 2023-02-15 12:52:31,499 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1653] 2023-02-15 12:52:31,499 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1654] 2023-02-15 12:52:31,499 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1655] 2023-02-15 12:52:31,499 >>   Gradient Accumulation steps = 16\n",
            "[INFO|trainer.py:1656] 2023-02-15 12:52:31,499 >>   Total optimization steps = 7461\n",
            "[INFO|trainer.py:1657] 2023-02-15 12:52:31,500 >>   Number of trainable parameters = 139420416\n",
            "  0% 0/7461 [00:00<?, ?it/s][WARNING|logging.py:281] 2023-02-15 12:52:31,533 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 12.0387, 'learning_rate': 0.00039951749095295543, 'epoch': 0.0}\n",
            "{'loss': 5.4927, 'learning_rate': 0.00039898136978957246, 'epoch': 0.0}\n",
            "{'loss': 5.174, 'learning_rate': 0.00039844524862618955, 'epoch': 0.0}\n",
            "{'loss': 5.034, 'learning_rate': 0.00039790912746280664, 'epoch': 0.01}\n",
            "{'loss': 5.0071, 'learning_rate': 0.0003973730062994237, 'epoch': 0.01}\n",
            "{'loss': 4.9904, 'learning_rate': 0.00039683688513604076, 'epoch': 0.01}\n",
            "{'loss': 4.9826, 'learning_rate': 0.00039630076397265785, 'epoch': 0.01}\n",
            "{'loss': 4.9552, 'learning_rate': 0.0003957646428092749, 'epoch': 0.01}\n",
            "{'loss': 4.9793, 'learning_rate': 0.000395228521645892, 'epoch': 0.01}\n",
            "{'loss': 4.9595, 'learning_rate': 0.00039469240048250906, 'epoch': 0.01}\n",
            "{'loss': 4.938, 'learning_rate': 0.00039415627931912615, 'epoch': 0.01}\n",
            "{'loss': 4.9163, 'learning_rate': 0.00039362015815574324, 'epoch': 0.02}\n",
            "{'loss': 4.9005, 'learning_rate': 0.00039308403699236027, 'epoch': 0.02}\n",
            "{'loss': 4.8869, 'learning_rate': 0.00039254791582897736, 'epoch': 0.02}\n",
            "{'loss': 4.8962, 'learning_rate': 0.00039201179466559445, 'epoch': 0.02}\n",
            "{'loss': 4.895, 'learning_rate': 0.0003914756735022115, 'epoch': 0.02}\n",
            "{'loss': 4.8831, 'learning_rate': 0.00039093955233882857, 'epoch': 0.02}\n",
            "{'loss': 4.9241, 'learning_rate': 0.0003904034311754457, 'epoch': 0.02}\n",
            "{'loss': 4.8582, 'learning_rate': 0.00038986731001206275, 'epoch': 0.03}\n",
            "{'loss': 4.731, 'learning_rate': 0.00038933118884867983, 'epoch': 0.03}\n",
            "{'loss': 4.7091, 'learning_rate': 0.0003887950676852969, 'epoch': 0.03}\n",
            "{'loss': 4.6342, 'learning_rate': 0.00038825894652191396, 'epoch': 0.03}\n",
            "{'loss': 4.6452, 'learning_rate': 0.00038772282535853104, 'epoch': 0.03}\n",
            "{'loss': 4.6553, 'learning_rate': 0.0003872403163114864, 'epoch': 0.03}\n",
            "{'loss': 4.6201, 'learning_rate': 0.0003867041951481035, 'epoch': 0.03}\n",
            "{'loss': 4.6186, 'learning_rate': 0.0003861680739847206, 'epoch': 0.03}\n",
            "{'loss': 4.6084, 'learning_rate': 0.0003856319528213376, 'epoch': 0.04}\n",
            "{'loss': 4.6809, 'learning_rate': 0.0003850958316579547, 'epoch': 0.04}\n",
            "{'loss': 4.6502, 'learning_rate': 0.0003845597104945718, 'epoch': 0.04}\n",
            "{'loss': 4.6454, 'learning_rate': 0.0003840235893311888, 'epoch': 0.04}\n",
            "{'loss': 4.6402, 'learning_rate': 0.00038348746816780596, 'epoch': 0.04}\n",
            "{'loss': 4.654, 'learning_rate': 0.00038295134700442305, 'epoch': 0.04}\n",
            "{'loss': 4.638, 'learning_rate': 0.00038246883795737836, 'epoch': 0.04}\n",
            "{'loss': 4.6176, 'learning_rate': 0.00038193271679399544, 'epoch': 0.05}\n",
            "{'loss': 4.6279, 'learning_rate': 0.00038139659563061253, 'epoch': 0.05}\n",
            "{'loss': 4.6423, 'learning_rate': 0.0003808604744672296, 'epoch': 0.05}\n",
            "{'loss': 4.608, 'learning_rate': 0.0003803243533038467, 'epoch': 0.05}\n",
            "{'loss': 4.6108, 'learning_rate': 0.0003797882321404638, 'epoch': 0.05}\n",
            "{'loss': 4.6255, 'learning_rate': 0.00037925211097708083, 'epoch': 0.05}\n",
            "{'loss': 4.6126, 'learning_rate': 0.0003787159898136979, 'epoch': 0.05}\n",
            "{'loss': 4.6065, 'learning_rate': 0.000378179868650315, 'epoch': 0.05}\n",
            "{'loss': 4.6247, 'learning_rate': 0.00037764374748693204, 'epoch': 0.06}\n",
            "{'loss': 4.6205, 'learning_rate': 0.00037716123843988745, 'epoch': 0.06}\n",
            "{'loss': 4.6383, 'learning_rate': 0.0003766251172765045, 'epoch': 0.06}\n",
            "{'loss': 4.6169, 'learning_rate': 0.0003760889961131216, 'epoch': 0.06}\n",
            "{'loss': 4.6219, 'learning_rate': 0.00037555287494973866, 'epoch': 0.06}\n",
            "{'loss': 4.6263, 'learning_rate': 0.0003750167537863557, 'epoch': 0.06}\n",
            "{'loss': 4.6196, 'learning_rate': 0.0003744806326229728, 'epoch': 0.06}\n",
            "{'loss': 4.6341, 'learning_rate': 0.0003739445114595899, 'epoch': 0.07}\n",
            "{'loss': 4.6228, 'learning_rate': 0.00037340839029620696, 'epoch': 0.07}\n",
            "  7% 500/7461 [28:28<6:37:57,  3.43s/it][INFO|trainer.py:2964] 2023-02-15 13:20:59,893 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2966] 2023-02-15 13:20:59,893 >>   Num examples = 2000\n",
            "[INFO|trainer.py:2969] 2023-02-15 13:20:59,894 >>   Batch size = 16\n",
            "[INFO|configuration_utils.py:543] 2023-02-15 13:20:59,903 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            "  0% 0/125 [00:00<?, ?it/s]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:01,205 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            "  2% 2/125 [00:01<01:06,  1.86it/s]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:02,273 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            "  2% 3/125 [00:02<01:40,  1.22it/s]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:03,499 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            "  3% 4/125 [00:03<02:06,  1.04s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:04,917 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            "  4% 5/125 [00:04<02:15,  1.13s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:06,204 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            "  5% 6/125 [00:06<02:33,  1.29s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:07,827 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            "  6% 7/125 [00:07<02:25,  1.24s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:08,935 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            "  6% 8/125 [00:08<02:17,  1.17s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:09,978 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            "  7% 9/125 [00:09<02:11,  1.14s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:11,032 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            "  8% 10/125 [00:10<02:06,  1.10s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:12,049 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            "  9% 11/125 [00:11<02:04,  1.09s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:13,113 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 10% 12/125 [00:13<02:09,  1.14s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:14,377 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 10% 13/125 [00:14<02:07,  1.14s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:15,505 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 11% 14/125 [00:15<02:01,  1.10s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:16,513 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 12% 15/125 [00:16<02:08,  1.17s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:17,858 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 13% 16/125 [00:17<02:11,  1.20s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:19,135 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 14% 17/125 [00:19<02:12,  1.22s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:20,402 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 14% 18/125 [00:20<02:06,  1.18s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:21,482 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 15% 19/125 [00:21<02:17,  1.30s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:23,058 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 16% 20/125 [00:22<02:11,  1.25s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:24,183 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 17% 21/125 [00:24<02:04,  1.19s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:25,249 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 18% 22/125 [00:25<01:58,  1.15s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:26,302 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 18% 23/125 [00:26<01:59,  1.18s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:27,533 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 19% 24/125 [00:27<02:01,  1.21s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:28,812 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 20% 25/125 [00:28<01:55,  1.16s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:29,855 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 21% 26/125 [00:30<02:01,  1.23s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:31,244 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 22% 27/125 [00:31<02:03,  1.27s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:32,599 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 22% 28/125 [00:32<02:03,  1.27s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:33,881 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 23% 29/125 [00:34<02:04,  1.29s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:35,236 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 24% 30/125 [00:35<02:01,  1.28s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:36,469 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 25% 31/125 [00:36<02:04,  1.33s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:37,926 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 26% 32/125 [00:37<02:00,  1.30s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:39,149 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 26% 33/125 [00:39<01:59,  1.30s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:40,435 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 27% 34/125 [00:40<01:49,  1.20s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:41,411 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 28% 35/125 [00:41<01:46,  1.18s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:42,550 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 29% 36/125 [00:42<01:57,  1.32s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:44,196 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 30% 37/125 [00:44<01:51,  1.27s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:45,331 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 30% 38/125 [00:45<01:48,  1.25s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:46,554 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 31% 39/125 [00:46<01:42,  1.19s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:47,614 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 32% 40/125 [00:47<01:46,  1.25s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:48,995 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 33% 41/125 [00:49<01:47,  1.28s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:50,344 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 34% 42/125 [00:50<01:49,  1.31s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:51,740 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 34% 43/125 [00:51<01:47,  1.31s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:53,030 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 35% 44/125 [00:53<01:47,  1.33s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:54,400 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 36% 45/125 [00:54<01:51,  1.40s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:55,959 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 37% 46/125 [00:56<01:50,  1.39s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:57,346 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 38% 47/125 [00:57<01:47,  1.38s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:21:58,700 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 38% 48/125 [00:58<01:45,  1.37s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:00,038 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 39% 49/125 [01:00<01:42,  1.35s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:01,326 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 40% 50/125 [01:01<01:38,  1.31s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:02,560 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 41% 51/125 [01:02<01:31,  1.23s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:03,605 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 42% 52/125 [01:03<01:25,  1.18s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:04,658 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 42% 53/125 [01:04<01:30,  1.26s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:06,121 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 43% 54/125 [01:06<01:28,  1.25s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:07,332 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 44% 55/125 [01:07<01:28,  1.27s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:08,643 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 45% 56/125 [01:08<01:27,  1.26s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:09,898 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 46% 57/125 [01:09<01:24,  1.24s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:11,083 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 46% 58/125 [01:11<01:25,  1.27s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:12,418 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 47% 59/125 [01:12<01:18,  1.19s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:13,425 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 48% 60/125 [01:13<01:10,  1.09s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:14,266 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 49% 61/125 [01:14<01:13,  1.15s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:15,571 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 50% 62/125 [01:15<01:10,  1.12s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:16,620 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 50% 63/125 [01:16<01:09,  1.12s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:17,735 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 51% 64/125 [01:17<01:09,  1.14s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:18,926 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 52% 65/125 [01:18<01:10,  1.17s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:20,158 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 53% 66/125 [01:20<01:12,  1.22s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:21,509 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 54% 67/125 [01:21<01:12,  1.25s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:22,810 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 54% 68/125 [01:22<01:08,  1.20s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:23,884 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 55% 69/125 [01:23<01:04,  1.15s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:24,923 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 56% 70/125 [01:24<01:02,  1.13s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:26,005 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 57% 71/125 [01:26<01:02,  1.16s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:27,223 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 58% 72/125 [01:27<01:01,  1.16s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:28,396 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 58% 73/125 [01:28<01:00,  1.16s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:29,557 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 59% 74/125 [01:29<00:56,  1.10s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:30,529 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 60% 75/125 [01:30<00:57,  1.15s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:31,786 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 61% 76/125 [01:31<00:54,  1.12s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:32,824 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 62% 77/125 [01:33<00:57,  1.20s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:34,219 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 62% 78/125 [01:34<00:57,  1.22s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:35,481 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 63% 79/125 [01:35<00:55,  1.20s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:36,657 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 64% 80/125 [01:36<00:52,  1.18s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:37,766 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 65% 81/125 [01:37<00:52,  1.19s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:38,975 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 66% 82/125 [01:39<00:52,  1.22s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:40,255 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 66% 83/125 [01:40<00:51,  1.23s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:41,505 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 67% 84/125 [01:41<00:50,  1.24s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:42,784 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 68% 85/125 [01:42<00:46,  1.17s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:43,779 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 69% 86/125 [01:43<00:45,  1.16s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:44,927 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 70% 87/125 [01:44<00:44,  1.16s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:46,085 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 70% 88/125 [01:45<00:41,  1.13s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:47,137 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 71% 89/125 [01:47<00:41,  1.16s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:48,372 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 72% 90/125 [01:48<00:40,  1.17s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:49,565 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 73% 91/125 [01:49<00:39,  1.17s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:50,725 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 74% 92/125 [01:50<00:40,  1.21s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:52,049 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 74% 93/125 [01:52<00:40,  1.28s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:53,468 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 75% 94/125 [01:53<00:38,  1.25s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:54,662 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 76% 95/125 [01:54<00:37,  1.23s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:55,850 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 77% 96/125 [01:55<00:34,  1.20s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:56,977 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 78% 97/125 [01:56<00:32,  1.16s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:58,044 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 78% 98/125 [01:58<00:31,  1.18s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:22:59,270 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 79% 99/125 [01:59<00:29,  1.14s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:00,311 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 80% 100/125 [02:00<00:29,  1.17s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:01,570 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 81% 101/125 [02:01<00:29,  1.25s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:02,982 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 82% 102/125 [02:03<00:28,  1.25s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:04,255 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 82% 103/125 [02:04<00:31,  1.45s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:06,167 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 83% 104/125 [02:06<00:28,  1.35s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:07,281 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 84% 105/125 [02:07<00:27,  1.40s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:08,797 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 85% 106/125 [02:08<00:25,  1.35s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:10,010 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 86% 107/125 [02:10<00:23,  1.31s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:11,243 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 86% 108/125 [02:11<00:21,  1.24s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:12,311 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 87% 109/125 [02:12<00:19,  1.20s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:13,433 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 88% 110/125 [02:13<00:17,  1.19s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:14,578 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 89% 111/125 [02:14<00:15,  1.14s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:15,602 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 90% 112/125 [02:15<00:14,  1.13s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:16,719 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 90% 113/125 [02:16<00:14,  1.21s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:18,107 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 91% 114/125 [02:18<00:13,  1.24s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:19,429 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 92% 115/125 [02:19<00:13,  1.31s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:20,914 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 93% 116/125 [02:21<00:12,  1.34s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:22,308 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 94% 117/125 [02:22<00:10,  1.32s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:23,572 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 94% 118/125 [02:23<00:09,  1.37s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:25,077 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 95% 119/125 [02:24<00:07,  1.27s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:26,103 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 96% 120/125 [02:26<00:06,  1.27s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:27,383 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 97% 121/125 [02:27<00:04,  1.21s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:28,433 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 98% 122/125 [02:28<00:03,  1.26s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:29,821 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 98% 123/125 [02:29<00:02,  1.23s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:30,973 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            " 99% 124/125 [02:31<00:01,  1.26s/it]\u001b[A[INFO|configuration_utils.py:543] 2023-02-15 13:23:32,314 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 4.5405378341674805, 'eval_cer': 0.9459808121889449, 'eval_runtime': 154.7465, 'eval_samples_per_second': 12.924, 'eval_steps_per_second': 0.808, 'epoch': 0.07}\n",
            "  7% 500/7461 [31:03<6:37:57,  3.43s/it]\n",
            "100% 125/125 [02:33<00:00,  1.26s/it]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:2709] 2023-02-15 13:23:34,641 >> Saving model checkpoint to ./models/my-bart-base-en-mix/checkpoint-500\n",
            "[INFO|configuration_utils.py:453] 2023-02-15 13:23:34,642 >> Configuration saved in ./models/my-bart-base-en-mix/checkpoint-500/config.json\n",
            "[INFO|configuration_utils.py:336] 2023-02-15 13:23:34,643 >> Configuration saved in ./models/my-bart-base-en-mix/checkpoint-500/generation_config.json\n",
            "[INFO|modeling_utils.py:1704] 2023-02-15 13:23:36,464 >> Model weights saved in ./models/my-bart-base-en-mix/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2160] 2023-02-15 13:23:36,465 >> tokenizer config file saved in ./models/my-bart-base-en-mix/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2167] 2023-02-15 13:23:36,465 >> Special tokens file saved in ./models/my-bart-base-en-mix/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 4.6248, 'learning_rate': 0.00037287226913282405, 'epoch': 0.07}\n",
            "{'loss': 4.6022, 'learning_rate': 0.00037233614796944114, 'epoch': 0.07}\n",
            "{'loss': 4.6227, 'learning_rate': 0.00037180002680605817, 'epoch': 0.07}\n",
            "{'loss': 4.6133, 'learning_rate': 0.00037126390564267526, 'epoch': 0.07}\n",
            "{'loss': 4.6041, 'learning_rate': 0.00037072778447929235, 'epoch': 0.07}\n",
            "{'loss': 4.5954, 'learning_rate': 0.0003701916633159094, 'epoch': 0.08}\n",
            "{'loss': 4.6173, 'learning_rate': 0.0003697091542688648, 'epoch': 0.08}\n",
            "{'loss': 4.6006, 'learning_rate': 0.0003691730331054819, 'epoch': 0.08}\n",
            "{'loss': 4.602, 'learning_rate': 0.0003686369119420989, 'epoch': 0.08}\n",
            "{'loss': 4.6268, 'learning_rate': 0.000368100790778716, 'epoch': 0.08}\n",
            "{'loss': 4.6346, 'learning_rate': 0.0003675646696153331, 'epoch': 0.08}\n",
            "{'loss': 4.6381, 'learning_rate': 0.0003670285484519501, 'epoch': 0.08}\n",
            "{'loss': 4.6538, 'learning_rate': 0.00036649242728856727, 'epoch': 0.08}\n",
            "{'loss': 4.6123, 'learning_rate': 0.0003659563061251843, 'epoch': 0.09}\n",
            "{'loss': 4.6101, 'learning_rate': 0.0003654201849618014, 'epoch': 0.09}\n",
            "{'loss': 4.6003, 'learning_rate': 0.0003648840637984185, 'epoch': 0.09}\n",
            "{'loss': 4.6065, 'learning_rate': 0.0003643479426350355, 'epoch': 0.09}\n",
            "{'loss': 4.6252, 'learning_rate': 0.0003638118214716526, 'epoch': 0.09}\n",
            "{'loss': 4.6005, 'learning_rate': 0.0003632757003082697, 'epoch': 0.09}\n",
            "{'loss': 4.6092, 'learning_rate': 0.0003627395791448867, 'epoch': 0.09}\n",
            "{'loss': 4.6016, 'learning_rate': 0.00036220345798150386, 'epoch': 0.1}\n",
            "{'loss': 4.6073, 'learning_rate': 0.00036166733681812095, 'epoch': 0.1}\n",
            "{'loss': 4.612, 'learning_rate': 0.000361131215654738, 'epoch': 0.1}\n",
            "{'loss': 4.6025, 'learning_rate': 0.0003605950944913551, 'epoch': 0.1}\n",
            "{'loss': 4.604, 'learning_rate': 0.00036005897332797216, 'epoch': 0.1}\n",
            "{'loss': 4.6059, 'learning_rate': 0.0003595228521645892, 'epoch': 0.1}\n",
            "{'loss': 4.5951, 'learning_rate': 0.0003589867310012063, 'epoch': 0.1}\n",
            "{'loss': 4.6301, 'learning_rate': 0.00035845060983782337, 'epoch': 0.1}\n",
            "{'loss': 4.616, 'learning_rate': 0.00035791448867444046, 'epoch': 0.11}\n",
            "{'loss': 4.6206, 'learning_rate': 0.00035737836751105755, 'epoch': 0.11}\n",
            "{'loss': 4.5963, 'learning_rate': 0.0003568422463476746, 'epoch': 0.11}\n",
            " 11% 818/7461 [49:12<6:05:04,  3.30s/it]Traceback (most recent call last):\n",
            "  File \"run_summarization.py\", line 708, in <module>\n",
            "    main()\n",
            "  File \"run_summarization.py\", line 627, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1543, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1793, in _inner_training_loop\n",
            "    if (\n",
            "KeyboardInterrupt\n",
            " 11% 818/7461 [49:15<6:40:04,  3.61s/it]\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m8UomVTfMQLm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}