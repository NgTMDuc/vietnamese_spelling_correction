{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggDnRC3ak7c4"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOnnQcc8lHa8",
        "outputId": "5275284d-200a-41ac-af17-70216c2dbf0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqeXtfqyIUkU",
        "outputId": "ffd8a14a-e74d-49ba-f4f8-6616f29ac551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Vietnamese-Corrector'...\n",
            "remote: Enumerating objects: 103, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 103 (delta 4), reused 8 (delta 3), pack-reused 93\u001b[K\n",
            "Receiving objects: 100% (103/103), 90.78 MiB | 18.75 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Updating files: 100% (27/27), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/bmd1905/Vietnamese-Corrector.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se-wX7sFIZFh",
        "outputId": "a1dea95f-cd67-4bdc-eef4-e63e013339b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Vietnamese-Corrector\n"
          ]
        }
      ],
      "source": [
        "cd /content/Vietnamese-Corrector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx8XQhZhIm_x",
        "outputId": "fa32583d-84b9-49e6-c77c-5a60e6667bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m826.4/826.4 KB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 KB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.0/384.0 KB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 KB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.7/139.7 KB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.5/94.5 KB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -r requirements.txt unidecode lion_pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-h4bw5slDvU"
      },
      "source": [
        "# Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_5pUcnZI0bk",
        "outputId": "1514d40a-4d2c-4f9d-f866-acdcb2f014da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-02-16 12:33:46.903202: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-16 12:33:48.152420: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-16 12:33:48.152513: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-16 12:33:48.152530: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Downloading (…)lve/main/config.json: 100% 1.72k/1.72k [00:00<00:00, 258kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:01<00:00, 807kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:01<00:00, 415kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 1.02MB/s]\n",
            "100% 479568/479568 [01:32<00:00, 5197.69it/s]\n"
          ]
        }
      ],
      "source": [
        "!python generate_dataset.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc5OTIB9lPog"
      },
      "source": [
        "## Copy files into Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O57MGm_R7x_L"
      },
      "outputs": [],
      "source": [
        "!cp /content/Vietnamese-Corrector/vi.csv /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LCHLhhW76Gs"
      },
      "outputs": [],
      "source": [
        "!cp /content/Vietnamese-Corrector/vi.test.csv /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTOIBjy976KC"
      },
      "outputs": [],
      "source": [
        "!cp /content/Vietnamese-Corrector/vi.train.csv /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lVZkPDwGY17R"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVp4GYazJIUO"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PWAu2rFucI_",
        "outputId": "60368fd9-c6dd-48ae-f653-e3cbf6d6ea01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trXPWAuZFmrB",
        "outputId": "c214ff24-164e-4d5e-b99d-dd6b062ce6a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-25 01:13:44.757824: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-25 01:13:45.626305: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-25 01:13:45.626430: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-25 01:13:45.626450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=10000,\n",
            "evaluation_strategy=steps,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=32,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-07,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./models/my-custom-vinai/runs/Feb25_01-13-49_cc9c3da68ad1,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=0.5,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=./models/my-custom-vinai/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./models/my-custom-vinai/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.builder:Using custom data configuration default-0b466e1ff187dcb1\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/csv\n",
            "INFO:datasets.builder:Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-0b466e1ff187dcb1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-0b466e1ff187dcb1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 12446.01it/s]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1838.40it/s]\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-0b466e1ff187dcb1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 436.29it/s]\n",
            "[INFO|configuration_utils.py:658] 2023-02-25 01:13:53,297 >> loading configuration file /content/drive/MyDrive/models/my-custom-vinai/config.json\n",
            "[INFO|configuration_utils.py:712] 2023-02-25 01:13:53,301 >> Model config MBartConfig {\n",
            "  \"_name_or_path\": \"/content/drive/MyDrive/models/my-custom-vinai\",\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"MBartForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_attention_heads\": 12,\n",
            "  \"decoder_ffn_dim\": 3072,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 12,\n",
            "  \"encoder_ffn_dim\": 3072,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 6,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"mbart\",\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": false,\n",
            "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64001\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1800] 2023-02-25 01:13:53,751 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:1800] 2023-02-25 01:13:53,751 >> loading file bpe.codes\n",
            "[INFO|tokenization_utils_base.py:1800] 2023-02-25 01:13:53,751 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1800] 2023-02-25 01:13:53,751 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1800] 2023-02-25 01:13:53,751 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils.py:426] 2023-02-25 01:13:55,742 >> Adding <mask> to the vocabulary\n",
            "[INFO|modeling_utils.py:2272] 2023-02-25 01:13:55,763 >> loading weights file /content/drive/MyDrive/models/my-custom-vinai/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:543] 2023-02-25 01:14:01,989 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2857] 2023-02-25 01:14:03,801 >> All model checkpoint weights were used when initializing MBartForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:2865] 2023-02-25 01:14:03,801 >> All the weights of MBartForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/models/my-custom-vinai.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.\n",
            "[INFO|configuration_utils.py:505] 2023-02-25 01:14:04,328 >> loading configuration file /content/drive/MyDrive/models/my-custom-vinai/generation_config.json\n",
            "[INFO|configuration_utils.py:543] 2023-02-25 01:14:04,328 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"transformers_version\": \"4.26.1\"\n",
            "}\n",
            "\n",
            "Running tokenizer on train dataset:   0% 0/194194 [00:00<?, ? examples/s]/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3581: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-0b466e1ff187dcb1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-476b445ddb0d6081.arrow\n",
            "Running tokenizer on validation dataset:   0% 0/2000 [00:00<?, ? examples/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-0b466e1ff187dcb1/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-0a42ae89e3a78e4e.arrow\n",
            "Downloading builder script: 100% 5.60k/5.60k [00:00<00:00, 3.96MB/s]\n",
            "[INFO|trainer.py:565] 2023-02-25 01:15:44,350 >> Using cuda_amp half precision backend\n",
            "[INFO|trainer.py:710] 2023-02-25 01:15:44,350 >> The following columns in the training set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:1650] 2023-02-25 01:15:44,356 >> ***** Running training *****\n",
            "[INFO|trainer.py:1651] 2023-02-25 01:15:44,356 >>   Num examples = 194194\n",
            "[INFO|trainer.py:1652] 2023-02-25 01:15:44,356 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1653] 2023-02-25 01:15:44,356 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1654] 2023-02-25 01:15:44,356 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "[INFO|trainer.py:1655] 2023-02-25 01:15:44,356 >>   Gradient Accumulation steps = 32\n",
            "[INFO|trainer.py:1656] 2023-02-25 01:15:44,356 >>   Total optimization steps = 759\n",
            "[INFO|trainer.py:1657] 2023-02-25 01:15:44,357 >>   Number of trainable parameters = 149972736\n",
            "{'loss': 0.114, 'learning_rate': 9.86824769433465e-08, 'epoch': 0.01}\n",
            "  2% 12/759 [00:29<28:07,  2.26s/it]"
          ]
        }
      ],
      "source": [
        "!python /content/Vietnamese-Corrector/run_summarization.py \\\n",
        "    --model_name_or_path /content/drive/MyDrive/models/my-custom-vinai \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --evaluation_strategy=\"steps\" \\\n",
        "    --eval_steps=10000 \\\n",
        "    --train_file /content/Vietnamese-Corrector/vi.train.csv \\\n",
        "    --validation_file /content/Vietnamese-Corrector/vi.test.csv \\\n",
        "    --output_dir ./models/my-custom-vinai/ \\\n",
        "    --overwrite_output_dir \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --gradient_accumulation_steps=32 \\\n",
        "    --learning_rate=\"1e-7\" \\\n",
        "    --num_train_epochs=0.5 \\\n",
        "    --predict_with_generate \\\n",
        "\t--logging_steps=\"10\" \\\n",
        "    --save_total_limit=\"2\" \\\n",
        "    --max_target_length=1024 \\\n",
        "    --max_source_length=1024 \\\n",
        "    --fp16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRwTi1ITlXeq"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QxlfzlsH0DYa"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fix_spelling = pipeline(\"text2text-generation\", model=\"/content/drive/MyDrive/models/my-custom-vinai\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fix_spelling(\"côn viec kin doanh thì rất khọ khan. \", max_length=1024))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trm4BPwHM2IC",
        "outputId": "8dd90ed5-5ab0-403d-8d40-1c9c9a9e8b73"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'Công việc kinh doanh thì rất khó khăn.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fix_spelling(\"Tôi , đang học AI ở trun tam AI vietnam . \", max_length=2048))\n",
        "print(fix_spelling(\"Nhưng sức huỷ divt của cơn bão mitch vẫn chưa thấm vào đâu lsovớithảm hoạ tại Bangladesh ăm 1970\", max_length=2048))\n",
        "print(fix_spelling(\"Lần này anh Phươngqyết xếp hàng mua bằng được 1 chiếc\", max_length=2048))\n",
        "print(fix_spelling(\"một số chuyen gia tài chính ngâSn hànG của Việt Nam cũng chung quan điểmnày\", max_length=2048))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opWEjdu7Mi7O",
        "outputId": "4d26f523-f6c1-48a5-e0ab-2e40985fb733"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'Tôi , đang học AI ở trung tâm AI vietnam .'}]\n",
            "[{'generated_text': 'Nhưng sức huỷ đất của cơn bão ghts vẫn chưa thấm vào đâu so với thảm hoạ tại Bangladesh năm 1970 .'}]\n",
            "[{'generated_text': 'Lần này anh Phương quyết xếp hàng mua bằng được 1 chiếc.'}]\n",
            "[{'generated_text': 'Một số chuyên gia tài chính ngân hàng của Việt Nam cũng chung quan điểm này.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4U5tT2ebH3f",
        "outputId": "570b9f50-6083-47f8-ef2c-f0ae4f8756ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'Tôi , đang học AI ở trun tam AI vietnam . .'}]\n",
            "[{'generated_text': 'Nhưng sức huỷ divt của cơn bão một vẫn chưa thấm vào đâu lo với thể hoạ tỺ�i Bangladesh ăm 1970.'}]\n",
            "[{'generated_text': 'Lần này, anh Phương Quyết xáp hàng mua, bằng được 1 chiệc.'}]\n",
            "[{'generated_text': 'Một số chuyền gia tài chính ngân hàng của Việt Nam cũng chung quan điểm này.'}]\n"
          ]
        }
      ],
      "source": [
        "print(fix_spelling(\"Tôi , đang học AI ở trun tam AI vietnam . \", max_length=2048))\n",
        "print(fix_spelling(\"Nhưng sức huỷ divt của cơn bão mitch vẫn chưa thấm vào đâu lsovớithảm hoạ tại Bangladesh ăm 1970\", max_length=2048))\n",
        "print(fix_spelling(\"Lần này anh Phươngqyết xếp hàng mua bằng được 1 chiếc\", max_length=2048))\n",
        "print(fix_spelling(\"một số chuyen gia tài chính ngâSn hànG của Việt Nam cũng chung quan điểmnày\", max_length=2048))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gA6fHcaFxZ7"
      },
      "source": [
        "# Customize Creaet Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8UomVTfMQLm"
      },
      "outputs": [],
      "source": [
        "from lib2to3.pgen2.tokenize import tokenize\n",
        "import random \n",
        "from string import ascii_letters, punctuation, digits\n",
        "import re\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "def tokenizer_check_if_text_too_long(text, tokenizer, max_length):\n",
        "    data = tokenizer.batch_encode_plus([text],max_length=max_length,truncation=True,return_overflowing_tokens=True )    \n",
        "    if len(data[\"input_ids\"]) > 1:\n",
        "        return True\n",
        "    else:\n",
        "        return False#, len(data[\"input_ids\"][0])\n",
        "\n",
        "def delete_characters(text, char_delete_percentage=0.005):\n",
        "    modifyed_line = []   \n",
        "    for char in text:\n",
        "        if random.random() > char_delete_percentage or char in digits:\n",
        "            modifyed_line.append(char)\n",
        "    return \"\".join(modifyed_line)\n",
        "\n",
        "def insert_characters(text, augmentation_probability=0.01):\n",
        "    modifyed_line = []   \n",
        "    for char in text:\n",
        "        if random.random() <= augmentation_probability and char not in digits:            \n",
        "            modifyed_line.append(random.choice(ascii_letters))\n",
        "        modifyed_line.append(char)\n",
        "    return \"\".join(modifyed_line)\n",
        "\n",
        "def replace_characters(text, augmentation_probability=0.01):\n",
        "    modifyed_line = []   \n",
        "    for char in text:\n",
        "        if random.random() <= augmentation_probability and char not in digits:            \n",
        "            modifyed_line.append(random.choice(ascii_letters))\n",
        "        else:\n",
        "            modifyed_line.append(char)\n",
        "    return \"\".join(modifyed_line)\n",
        "\n",
        "def swap_characters_case(text, augmentation_probability=0.01):\n",
        "    modifyed_line = []   \n",
        "    for char in text:\n",
        "        if random.random() <= augmentation_probability:            \n",
        "            char = char.swapcase()\n",
        "        modifyed_line.append(char)\n",
        "    return \"\".join(modifyed_line)\n",
        "\n",
        "def lower_case_words(text, augmentation_probability=0.05):\n",
        "    modifyed_line = []   \n",
        "    for word in text.split():\n",
        "        if word[0].islower() == False and random.random() <= augmentation_probability:            \n",
        "            word = word.lower()\n",
        "        modifyed_line.append(word)\n",
        "    return \" \".join(modifyed_line)\n",
        "\n",
        "\n",
        "clean_chars = re.compile(r'[^A-Za-zöäüÖÄÜß,.!?’\\'$%€0-9\\(\\)\\- ]', re.MULTILINE)\n",
        "def cleanup(text):    \n",
        "    text = clean_chars.sub('', text)\n",
        "    #print(\"bug: somehow all numbers are removed - this is might be due to this regex\")\n",
        "    #exit()\n",
        "    #text = text.replace(\"\\n\", \"\")\n",
        "    #text = text.replace('\"','\\\\\"')\n",
        "    return text\n",
        "\n",
        "clean_punctuation = re.compile(r\"(?<!\\d)[.,;:'?!](?!\\d)\")\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Remove all punctuation from string, except if it's between digits\"\"\"\n",
        "    return clean_punctuation.sub(\"\", text)\n",
        "\n",
        "def combine_sentences(text, sentences, augmentation_probability = 1):\n",
        "    if random.random() < augmentation_probability:\n",
        "        sentences_to_sample = random.randint(0,10)\n",
        "        augmentation_sentences = random.sample(sentences,sentences_to_sample)    \n",
        "        return text + \" \" + \" \".join(augmentation_sentences)\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def delete_word(text, augmentation_probability = 0.001):        \n",
        "    if random.random() < augmentation_probability:\n",
        "        words = text.split()\n",
        "        if len(words) < 3:\n",
        "            # do not delete word in short text, as there will be no context to guess the word\n",
        "            return text\n",
        "        word_to_remove = random.randint(0,len(words)-1)\n",
        "        words.pop(word_to_remove)\n",
        "        return \" \".join(words)\n",
        "    else:\n",
        "        return text\n",
        "#=========================================================================\n",
        "import unidecode\n",
        "import numpy as np\n",
        "\n",
        "chars_regrex = '[aàảãáạăằẳẵắặâầẩẫấậAÀẢÃÁẠĂẰẲẴẮẶÂẦẨẪẤẬoòỏõóọôồổỗốộơờởỡớợOÒỎÕÓỌÔỒỔỖỐỘƠỜỞỠỚỢeèẻẽéẹêềểễếệEÈẺẼÉẸÊỀỂỄẾỆuùủũúụưừửữứựUÙỦŨÚỤƯỪỬỮỨỰiìỉĩíịIÌỈĨÍỊyỳỷỹýỵYỲỶỸÝỴnNvVmMCG]'\n",
        "same_chars = {\n",
        "    'a': ['á', 'à', 'ả', 'ã', 'ạ', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ'],\n",
        "    'A': ['Á','À','Ả','Ã','Ạ','Ấ','Ầ','Ẩ','Ẫ','Ậ','Ắ','Ằ','Ẳ','Ẵ','Ặ'],\n",
        "    'O': ['Ó','Ò','Ỏ','Õ','Ọ','Ô','Ố','Ồ','Ổ','Ỗ','Ộ','Ơ','Ớ','Ờ','Ở','Ỡ','Ợ','Q'],\n",
        "    'o': ['ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ','ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'q'],\n",
        "    'e': ['é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ê'],\n",
        "    'E': ['É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ê'],\n",
        "    'u': ['ú', 'ù', 'ủ', 'ũ', 'ụ', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'ư'],\n",
        "    'U': ['Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Ư'],\n",
        "    'i': ['í', 'ì', 'ỉ', 'ĩ', 'ị'],\n",
        "    'I': ['Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị'],\n",
        "    'y': ['ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'v'],\n",
        "    'Y': ['Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'V'],\n",
        "    'n': ['m'],\n",
        "    'N': ['N'],\n",
        "    'v': ['y'],\n",
        "    'V': ['Y'],\n",
        "    'm': ['n'],\n",
        "    'M': ['N'],\n",
        "    'C': ['G'],\n",
        "    'G': ['C']\n",
        "}\n",
        "def _char_regrex(text):\n",
        "    match_chars = re.findall(chars_regrex, text)\n",
        "    return match_chars\n",
        "def _random_replace(text, match_chars):\n",
        "    replace_char = match_chars[np.random.randint(low=0, high=len(match_chars), size=1)[0]]\n",
        "    insert_chars = same_chars[unidecode.unidecode(replace_char)]\n",
        "    insert_char = insert_chars[np.random.randint(low=0, high=len(insert_chars), size=1)[0]]\n",
        "    text = text.replace(replace_char, insert_char, 1)\n",
        "\n",
        "    return text\n",
        "def change(text):\n",
        "    match_chars = _char_regrex(text)\n",
        "    if len(match_chars) == 0:\n",
        "        return text\n",
        "\n",
        "    text = _random_replace(text, match_chars)\n",
        "\n",
        "    return text\n",
        "def replace_accent_chars(text, ratio=0.01):\n",
        "    words = text.split()\n",
        "    mask = np.random.random(size=len(words)) < ratio\n",
        "\n",
        "    for i in range(len(words)):\n",
        "        if mask[i]:\n",
        "            words[i] = change(words[i])\n",
        "            break          \n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def remove_random_accent(text, ratio=0.05):\n",
        "        words = text.split()\n",
        "        mask = np.random.random(size=len(words)) < ratio\n",
        "        \n",
        "        for i in range(len(words)):\n",
        "            if mask[i]:\n",
        "                words[i] = unidecode.unidecode(words[i])\n",
        "                break\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "# Space between words\n",
        "def remove_random_space(text):\n",
        "    words = text.split()\n",
        "    n_words = len(words)\n",
        "    start = np.random.randint(low=0, high=n_words, size=1)[0]\n",
        "\n",
        "    if start + 3 < n_words:\n",
        "        end = np.random.randint(low=start, high=start + 3, size=1)[0]\n",
        "    else:\n",
        "        end = np.random.randint(low=start, high=n_words, size=1)[0]\n",
        "\n",
        "    out = ' '.join(words[:start])  + ' ' + ''.join(words[start:end + 1]) + ' ' + ' '.join(words[end + 1:])\n",
        "\n",
        "    return out.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxWdpGjvFceY",
        "outputId": "fedeaca8-a0ef-4210-a8f0-140944f3e266"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "100%|██████████| 49062/49062 [00:26<00:00, 1844.92it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_file = \"/content/Vietnamese-Corrector/data/data50k.vi.txt\" #\"data/en.wikidump.processed.24m.txt\" #\n",
        "language = \"vi\"\n",
        "num_lines = sum(1 for line in open(data_file, 'r'))\n",
        "\n",
        "with open(data_file, 'r') as file:\n",
        "    sentences = file.readlines()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\") # for vi\n",
        "#xlm-roberta-base\n",
        "#facebook/bart-base\n",
        "#, encoding=\"utf-8\"\n",
        "count = 0\n",
        "datas = []\n",
        "with open(language+\".csv\", \"w\") as output:        \n",
        "    with open(data_file,'r') as file:\n",
        "        for line in tqdm(file, total=num_lines):\n",
        "            # if count == 0:\n",
        "            #   count += 1\n",
        "            #   continue\n",
        "            #line = cleanup(line)\n",
        "            if len(line) < 1:\n",
        "                continue \n",
        "\n",
        "            line = line.replace('\"', '')\n",
        "            \n",
        "            #line = combine_sentences(line,sentences)\n",
        "            # print()\n",
        "            # print(len(line), line)              \n",
        "            if tokenizer_check_if_text_too_long(line,tokenizer,max_length=1024):\n",
        "                #print(f\"skipping line as its too long ({len(line)}):\\n\"+line)\n",
        "                continue\n",
        "            \n",
        "            if random.random() > 0.02:\n",
        "                # we will leave 2% of the data untouched, to teach the \n",
        "                # model, not to \"overact\" on the texts\n",
        "                # my custom\n",
        "                new_line = remove_random_space(line)\n",
        "                new_line = remove_random_accent(new_line)\n",
        "                new_line = replace_accent_chars(new_line)\n",
        "\n",
        "                # original\n",
        "                new_line = swap_characters_case(new_line)  \n",
        "                new_line = delete_word(new_line)                    \n",
        "                new_line = delete_characters(new_line)\n",
        "                new_line = insert_characters(new_line)\n",
        "                new_line = replace_characters(new_line)\n",
        "                new_line = lower_case_words(new_line)                                           \n",
        "                new_line = remove_punctuation(new_line)\n",
        "            else:\n",
        "                new_line = line          \n",
        "            # print(len(new_line), new_line)\n",
        "            # count += 1\n",
        "            # break\n",
        "            # if count == 1:\n",
        "            #     break\n",
        "            datas.append([new_line.strip(), line.strip()])\n",
        "            #count += 1\n",
        "            # if count == 10:\n",
        "            #   break\n",
        "            output.write(f'\"{new_line.strip()}\",\"{line.strip()}\"\\n')        \n",
        "os.system(f\"echo \\\"text,summary\\\" > {language}.train.csv\")\n",
        "num_lines = sum(1 for line in open(f\"{language}.csv\",'r'))\n",
        "os.system(f\"head -n {num_lines-2000} {language}.csv >> {language}.train.csv\")\n",
        "os.system(f\"echo \\\"text,summary\\\" > {language}.test.csv\")\n",
        "os.system(f\"tail -n 2000 {language}.csv >> {language}.test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFEd3gagH6LB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}