{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ogu/source/experiments/spelling_bert/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_lightning as pl\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import random\n",
    "from string import ascii_letters\n",
    "from datasets import Dataset\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning_bert import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_text(items):\n",
    "    result = tokenizer(list(\n",
    "        map(replace_augment, items[\"input\"])), truncation=True, padding=\"max_length\")\n",
    "        \n",
    "    target = tokenizer(\n",
    "        items['input'], truncation=True, padding=\"max_length\")\n",
    "    result['target_ids'] = target[\"input_ids\"]\n",
    "    result['target_attention_mask'] = target[\"attention_mask\"]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0:   0%|          | 0/3 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "#0:  33%|███▎      | 1/3 [00:00<00:01,  1.24ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "#0:  67%|██████▋   | 2/3 [00:01<00:00,  1.27ba/s]\n",
      "#2: 100%|██████████| 3/3 [00:01<00:00,  1.54ba/s]\n",
      "\n",
      "\n",
      "#3: 100%|██████████| 3/3 [00:01<00:00,  1.54ba/s]\n",
      "#1: 100%|██████████| 3/3 [00:01<00:00,  1.53ba/s]\n",
      "#0: 100%|██████████| 3/3 [00:01<00:00,  1.52ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "texts = load_data(\"en.txt\")\n",
    "texts = [text.lower() for text in texts]\n",
    "my_dict = {\"input\":texts}\n",
    "dataset = Dataset.from_dict(my_dict)\n",
    "dataset = dataset.map(map_text, batched=True,num_proc=4)\n",
    "dataset = dataset.remove_columns(\"input\")\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'target_ids', 'target_attention_mask'])\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataloader = DataLoader(dataset[\"train\"], shuffle=True, batch_size=8)\n",
    "test_dataloader = DataLoader(dataset[\"test\"], batch_size=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | BertForMaskedLM  | 109 M \n",
      "1 | loss  | CrossEntropyLoss | 0     \n",
      "-------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "219.029   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  6.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ogu/source/experiments/spelling_bert/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/ogu/source/experiments/spelling_bert/venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ogu/source/experiments/spelling_bert/venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1125/1125 [03:58<00:00,  4.71it/s, loss=2.02, v_num=21]\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = SpellingBert()\n",
    "\n",
    "#logger\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "# training\n",
    "trainer = pl.Trainer(logger=logger,gpus=1, precision=16, max_epochs=2)# ,limit_train_batches=0.5\n",
    "trainer.fit(model, train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] my family is is. [SEP]\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "\n",
    "inputs = tokenizer(\"my nahme is peter\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model.model(**inputs).logits\n",
    "\n",
    "predicted_token_ids = logits.argmax(axis=-1)\n",
    "print(tokenizer.decode(predicted_token_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2026,  2171,  2003, 11754,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tokenizer(\"my name is slim\", return_tensors=\"pt\",add_special_tokens=True)#\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] my name is slim [SEP]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(a['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 2026,  2171,  2003, 11754]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"my name is slim\", return_tensors=\"pt\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#facebook/bart-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/25/2023 08:09:57 - WARNING - __main__ - Process rank: -1, device: mps, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/25/2023 08:09:57 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=5000,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./models/test_model/runs/Feb25_08-09-57_Henrys-MacBook-Pro.local,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=./models/test_model/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./models/test_model/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=True,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "02/25/2023 08:09:58 - INFO - datasets.builder - Using custom data configuration default-6e52c78532a2580e\n",
      "02/25/2023 08:09:58 - INFO - datasets.info - Loading Dataset Infos from /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages/datasets/packaged_modules/csv\n",
      "02/25/2023 08:09:58 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "02/25/2023 08:09:58 - INFO - datasets.info - Loading Dataset info from /Users/bmd1905/.cache/huggingface/datasets/csv/default-6e52c78532a2580e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
      "02/25/2023 08:09:58 - WARNING - datasets.builder - Found cached dataset csv (/Users/bmd1905/.cache/huggingface/datasets/csv/default-6e52c78532a2580e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "02/25/2023 08:09:58 - INFO - datasets.info - Loading Dataset info from /Users/bmd1905/.cache/huggingface/datasets/csv/default-6e52c78532a2580e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 87.65it/s]\n",
      "[INFO|configuration_utils.py:654] 2023-02-25 08:10:00,082 >> loading configuration file config.json from cache at /Users/bmd1905/.cache/huggingface/hub/models--vinai--bartpho-word-base/snapshots/558969fe5aeaabfbd2eb5a59ab4908803cd9788d/config.json\n",
      "[INFO|configuration_utils.py:706] 2023-02-25 08:10:00,091 >> Model config MBartConfig {\n",
      "  \"_name_or_path\": \"vinai/bartpho-word-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"MBartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2023-02-25 08:10:02,148 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2023-02-25 08:10:04,236 >> loading configuration file config.json from cache at /Users/bmd1905/.cache/huggingface/hub/models--vinai--bartpho-word-base/snapshots/558969fe5aeaabfbd2eb5a59ab4908803cd9788d/config.json\n",
      "[INFO|configuration_utils.py:706] 2023-02-25 08:10:04,236 >> Model config MBartConfig {\n",
      "  \"_name_or_path\": \"vinai/bartpho-word-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"MBartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1775] 2023-02-25 08:10:05,431 >> loading file vocab.txt from cache at /Users/bmd1905/.cache/huggingface/hub/models--vinai--bartpho-word-base/snapshots/558969fe5aeaabfbd2eb5a59ab4908803cd9788d/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1775] 2023-02-25 08:10:05,431 >> loading file bpe.codes from cache at /Users/bmd1905/.cache/huggingface/hub/models--vinai--bartpho-word-base/snapshots/558969fe5aeaabfbd2eb5a59ab4908803cd9788d/bpe.codes\n",
      "[INFO|tokenization_utils_base.py:1775] 2023-02-25 08:10:05,431 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2023-02-25 08:10:05,431 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2023-02-25 08:10:05,431 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2023-02-25 08:10:05,432 >> loading configuration file config.json from cache at /Users/bmd1905/.cache/huggingface/hub/models--vinai--bartpho-word-base/snapshots/558969fe5aeaabfbd2eb5a59ab4908803cd9788d/config.json\n",
      "[INFO|configuration_utils.py:706] 2023-02-25 08:10:05,433 >> Model config MBartConfig {\n",
      "  \"_name_or_path\": \"vinai/bartpho-word-base\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"MBartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils.py:426] 2023-02-25 08:10:05,506 >> Adding <mask> to the vocabulary\n",
      "[WARNING|logging.py:281] 2023-02-25 08:10:05,506 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|modeling_utils.py:2158] 2023-02-25 08:10:05,517 >> loading weights file pytorch_model.bin from cache at /Users/bmd1905/.cache/huggingface/hub/models--vinai--bartpho-word-base/snapshots/558969fe5aeaabfbd2eb5a59ab4908803cd9788d/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2608] 2023-02-25 08:10:07,007 >> All model checkpoint weights were used when initializing MBartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:2616] 2023-02-25 08:10:07,007 >> All the weights of MBartForConditionalGeneration were initialized from the model checkpoint at vinai/bartpho-word-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.\n",
      "02/25/2023 08:10:07 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /Users/bmd1905/.cache/huggingface/datasets/csv/default-6e52c78532a2580e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-e393e943b5b67ee6.arrow\n",
      "02/25/2023 08:10:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /Users/bmd1905/.cache/huggingface/datasets/csv/default-6e52c78532a2580e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-990e650d60b9445e.arrow\n",
      "[INFO|trainer.py:725] 2023-02-25 08:10:11,283 >> The following columns in the training set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:1608] 2023-02-25 08:10:11,286 >> ***** Running training *****\n",
      "[INFO|trainer.py:1609] 2023-02-25 08:10:11,286 >>   Num examples = 194194\n",
      "[INFO|trainer.py:1610] 2023-02-25 08:10:11,286 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1611] 2023-02-25 08:10:11,286 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1612] 2023-02-25 08:10:11,286 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1613] 2023-02-25 08:10:11,286 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1614] 2023-02-25 08:10:11,286 >>   Total optimization steps = 388388\n",
      "[INFO|trainer.py:1615] 2023-02-25 08:10:11,286 >>   Number of trainable parameters = 149972736\n",
      "{'loss': 4.3192, 'learning_rate': 9.999742525515721e-07, 'epoch': 0.0}          \n",
      "  0%|                                    | 10/388388 [00:10<90:29:08,  1.19it/s]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bmd1905/Desktop/spell/spelling/run_summarization.py\", line 711, in <module>\n",
      "    main()\n",
      "  File \"/Users/bmd1905/Desktop/spell/spelling/run_summarization.py\", line 630, in main\n",
      "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "  File \"/Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages/transformers/trainer.py\", line 1501, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages/transformers/trainer.py\", line 1749, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages/transformers/trainer.py\", line 2508, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages/transformers/trainer.py\", line 2540, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1488, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages/transformers/models/mbart/modeling_mbart.py\", line 1375, in forward\n",
      "    masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
      "  File \"/Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1488, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages/torch/nn/modules/loss.py\", line 1174, in forward\n",
      "    return F.cross_entropy(input, target, weight=self.weight,\n",
      "  File \"/Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages/torch/nn/functional.py\", line 3029, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
      "KeyboardInterrupt\n",
      "  0%|                                   | 10/388388 [00:10<115:22:31,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "!python run_summarization.py \\\n",
    "    --model_name_or_path vinai/bartpho-word-base \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --evaluation_strategy=\"steps\" \\\n",
    "    --eval_steps=5000 \\\n",
    "    --train_file vi.train.csv \\\n",
    "    --validation_file vi.test.csv \\\n",
    "    --output_dir ./models/test_model/ \\\n",
    "    --overwrite_output_dir \\\n",
    "    --per_device_train_batch_size=1 \\\n",
    "    --per_device_eval_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=1 \\\n",
    "    --num_train_epochs=2 \\\n",
    "    --predict_with_generate \\\n",
    "\t--logging_steps=\"10\" \\\n",
    "    --save_total_limit=\"2\" \\\n",
    "    --max_target_length=128 \\\n",
    "    --max_source_length=128 \\\n",
    "    --learning_rate=\"1e-6\" \\\n",
    "    --use_mps_device \\\n",
    "    #--fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lion_pytorch\n",
      "  Downloading lion_pytorch-0.0.7-py3-none-any.whl (4.3 kB)\n",
      "Requirement already satisfied: transformers in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (4.24.0)\n",
      "Requirement already satisfied: datasets in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (2.10.0)\n",
      "Requirement already satisfied: evaluate in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: pip in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (23.0.1)\n",
      "Requirement already satisfied: install in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (1.3.5)\n",
      "Requirement already satisfied: jiwer in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: torch>=1.6 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from lion_pytorch) (2.0.0.dev20230201)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: filelock in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: aiohttp in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pandas in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from datasets) (1.5.2)\n",
      "Requirement already satisfied: responses<0.19 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: levenshtein==0.20.2 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from jiwer) (0.20.2)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from levenshtein==0.20.2->jiwer) (2.13.7)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from torch>=1.6->lion_pytorch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from torch>=1.6->lion_pytorch) (2.8.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/bmd1905/miniforge3/envs/ocr/lib/python3.9/site-packages (from sympy->torch>=1.6->lion_pytorch) (1.2.1)\n",
      "Installing collected packages: lion_pytorch\n",
      "Successfully installed lion_pytorch-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install lion_pytorch transformers datasets evaluate pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7a843eca1cab639b36c3fbecef55435faf57387ff90456c4ff67b93cd6a8d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
